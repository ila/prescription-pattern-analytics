\chapter{Goals definition} 
\section{Methodologies}
A general practitioner, according to WONCA (World Organization of Family Doctors)  is responsible of supply of integrated and continuative care. Some fundamental skills and activities\cite{wonka} to pursue this goal are:
\begin{itemize}
	\item Communication with patients;
	\item Management of the practice;
	\item Clinical tasks;
	\item Problem solving;
	\item Holistic modelling.
\end{itemize}

This research is specifically focussed on the doctor-patient relationship: person-centred care concerning diagnoses and prescriptions, analysing their changes according to habits, physiological aspects, time and geographical area of both interested parts. 

There are several external factors influencing market trends of medicines, for instance the advent of generic drugs, having the same active principle and bioequivalence with brand medicines, but lower prices thanks to their dissociation from pharmaceutical companies.

The concept of generic drug has been introduced in Italy in 1995, to be legally formalised in 1996\cite{generico}, yet it has initially been perceived by general practitioners and pharmacists as a mere instrument to save money at the expense of quality\cite{medicinaliequivalenti}.

Only in the past 10 years, advertisement efforts have been made to make the population aware of the strict quality checks and the reliability of generic drugs, starting a slow switching process. If sales of a brand product decrease, then, it might have been replaced with its equivalent.

This is only an instance of event which cannot be predicted using the raw data: information about market withdrawals, advertisement or economic availability need to be cross-checked with analytical results.

Considering a wide time span is essential to have an overall idea of data quality, information loss and potentiality of available resources. All those factors contribute to progressive rescaling of the dataset:
\begin{enumerate}
	\item Detailed analytics must be done according to restricted areas (pathologies, products);
	\item Different time spans can be compared, going deeper into the general view;
	\item Incorrect or unclear information has to be removed, causing retention of total records whose amount gets narrower while cleaning is in progress (funnel).
\end{enumerate}

The dataset is modelled in a relational schema, which allows organization of records in structures (tables) to maintain integrity and compatibility between different kind of fields. This allows flexibility using dynamic views and query optimisation based on set theory. 

For a better fruition of the workload, table names have been changed to standard English keywords.

\section{Considerations}
Since health big data can provide a high variety of information, it is required to have some clear \textit{objectives} in mind to keep on track and avoid losing focus. 

The research is centred on the \textbf{changes of prescription patterns of antibiotics}: this is a wide goal, and more information is needed to achieve results. It's necessary to narrow the field down, only concentrating on some classes of medicines, and define subgroups of GPs and patients in a restricted amount of time.

To obtain constraints the more objective as possible, some further analysis can be useful. For instance, focussing on chronic patients is a relatively fast way to reduce the huge amount of rows to elaborate, but causes the loss of most information.

The first step to take, having a deep understanding of the data, is recognising the extent and impact of the \textbf{progressive information loss}, to define the final amount of clear records. Trying to fix mistakes is a risk, since the outcome could be incorrect, so deleting is the most practical option.

Removing unclear and futile data may not be enough to have consistent results: 18 years of data is a wide range, and \textit{splitting the dataset} or deciding to only consider a smaller scope can be beneficial for the analysis quality. 

Some constraints can be imposed on general practitioners as well: since the results have to be coherent and accurate, it's best practice to only consider \textbf{active GPs} with a \textbf{constant number of patients} (to be defined). \\ This would partially remedy the fact that doctors may have different approaches to the same disease.

The creation of cohorts (chronic patients) among the statistical population is an example of \textbf{cluster sampling}.

After the initial parsing, there will be a rough draft of the final result which then will be subject of the following steps:
\begin{enumerate}
	\item Further analysis on data correctness (record linkage);
	\item Elaboration of the statistics and time series clustering.
\end{enumerate}

Another relevant instance for analysis is the \textbf{subset} of diseases to consider: choices have to be made according to \textit{external studies}, \textit{marketing researches} and further \textit{discoveries on the provided data}. Focussing on the \textbf{most common ones} is a guideline to start.

Having an idea of which illnesses and prescription have unstable patterns might give a better vision, and can be done through statistics on the whole database. 

Some examples of analytics are:
\begin{itemize}
	\item Most common diseases through the years;
	\item Most common \textit{chronic} diseases through the years;
	\item Changes of the number of prescriptions for diseases in the same area;
	\item Changes of prescriptions based on the patient phenotype or market trends.
\end{itemize}

An obstacle to perceiving the meaning of results is the restricted domain knowledge: to compensate, confronting some experts in the field is required. The team comprehends computer scientists, statisticians, biologists and healthcare workers.

\section{Practical goals }
Having a better vision of the medical domain and the rising issues in the Italian system, before approaching analytical procedures it is necessary to underline practical objectives and expected outcomes.

The first essential statistic to extract is about progressive information loss: this makes possible to estimate how much data can give reliable and complete results, compared to the total. It is defined \textit{progressive} since the more iterations of cleaning are being made, the more data is going to be lost. 

Parsing remaining data is essential to have a correct functioning of database interrogations. Fields need to be checked for correctness using lookup tables, record linkage or regular expression matching; text has to be cast to numbers to apply mathematical operations, and dates can be divided into months and years.

Performances require a high level of optimisation due to the huge workload of information, obtained through targeted expressions, strict constraints and better memory management of records. 

% finire

\section{Tools} % finire
The tools to analyse and elaborate the health data are:
\begin{itemize}
	\item \textbf{PostgreSQL}, for data management and querying. 
	\begin{itemize}
		\item The development platform to interface with the web server is \textbf{PgAdmin 4};
		\item All queries need to be optimized to avoid huge computational times, using indexes and Common Table Expressions;
	\end{itemize}
	\item \textbf{Neo4j}, che mi sta dando l'inferno;
	\item \textbf{R}, for machine learning and graphics computing. 
	\begin{itemize}
		\item It offers many ways to create plots;
		\item Possible uses are time series analysis and trajectory clustering.
	\end{itemize}
\end{itemize}

Reports and slide-shares have been created and accessed using the \textbf{Google Suite}.

Due to the amount of sensitive data, detailed results are going to be omitted: the final conclusions will be a product of aggregation and schematisation.
